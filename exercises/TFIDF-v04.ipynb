{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "from string import punctuation\n",
    "import re\n",
    "import regex\n",
    "tokenizer = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopW = stopwords.words('english')\n",
    "# stopW = ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "# print(sorted(stopW))\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histologic section of adrenal metastatic disease (hematoxylin and eosin, Г—100). Tumor cells arranged in solid nests infiltrating lymph node tissue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'articleId': 'PMC7282154',\n",
       " 'image_path': 'PMC7282154-6_i2376-0605-6-2-e50-f06.jpg',\n",
       " 'image_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7282154/bin/i2376-0605-6-2-e50-f06.jpg',\n",
       " 'articleUrl': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7282154/',\n",
       " 'articleTitle': 'RECURRENT INVASIVE DUCTAL BREAST CARCINOMA PRESENTING AS PRIMARY ADRENAL INSUFFICIENCY WITH ADRENAL CRISIS',\n",
       " 'caption': 'Histologic section of adrenal metastatic disease (hematoxylin and eosin, Г—100). Tumor cells arranged in solid nests infiltrating lymph node tissue.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### a method to Read a json file with img captions and article titles and parse it into set of documents\n",
    "\n",
    "def getDocs(sourcePath):\n",
    "    medDocs = []\n",
    "    sourceFile = open(sourcePath, 'r')\n",
    "\n",
    "    count = 1\n",
    "    for line in sourceFile:\n",
    "        medDocs.append(json.loads(line))\n",
    "        count += 1\n",
    "\n",
    "    sourceFile.close()\n",
    "    \n",
    "    # for k in range(1,count):\n",
    "    #     print(str(k) + \" : \" + medDocs[k]['docId'])\n",
    "    return medDocs\n",
    "\n",
    "\n",
    "sourcePath = 'assets/medImages.jl'\n",
    "medDocs = getDocs(sourcePath)\n",
    "print(medDocs[262]['caption'])\n",
    "medDocs[262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method that cleans the title and caption text in each document\n",
    "# the result is tokenized titles and caption without unwanted brackets, numbers, spaces, tags, unicode charts;\n",
    "# result includes no punctuation and stopwards and tokens with length less than 2 symbols\n",
    "# all tokens are lower case\n",
    "\n",
    "\n",
    "def preprocessContent(content):\n",
    "    sentences = sent_tokenize(content)\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        # removes unicode\n",
    "        cleanedTokens = re.sub(r'[^\\x00-\\x7F]+', ' ', sent)\n",
    "        # removes menions\n",
    "        cleanedTokens = re.sub(r'@\\w+', '', cleanedTokens)\n",
    "        # removes numbers\n",
    "#         cleanedTokens = re.sub('[0-9]+', '', cleanedTokens)\n",
    "        # removes nested brackets \n",
    "        cleanedTokens = regex.sub(r'\\([^()]*+(?:(?R)[^()]*)*+\\)', '', cleanedTokens) \n",
    "        # removes nested currly brackets\n",
    "        cleanedTokens = regex.sub('\\{(?:[^}{]|\\{[^}{]*\\})*\\}', '', cleanedTokens) \n",
    "        # removes html tags\n",
    "        cleanedTokens = re.sub('\\<+/*\\w*/*\\>+', '', cleanedTokens)\n",
    "        # removes punctuation\n",
    "        cleanedTokens = re.sub(r'[%s]' % re.escape(punctuation), ' ', cleanedTokens)\n",
    "        # removes doubled spaces\n",
    "        cleanedTokens = re.sub(r'\\s{2,}', ' ', cleanedTokens)\n",
    "        sentTokens = tokenizer.tokenize(cleanedTokens)\n",
    "        # lower case each token and removes tokens with length less than 2\n",
    "        sentTokens = [tok.lower() for tok in sentTokens if len(tok) >2]\n",
    "        # removes stop words \n",
    "        sentTokens2 = [tok for tok in sentTokens if not tok.lower() in stopW ]\n",
    "        tokens += sentTokens2\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def cleanDocs(medDocs):\n",
    "    cleanedDocs = deepcopy(medDocs)\n",
    "    for i, doc in enumerate(cleanedDocs):\n",
    "        doc['articleTitle'] = preprocessContent(doc['articleTitle'] )\n",
    "        doc['caption'] = preprocessContent(doc['caption'])\n",
    "    return cleanedDocs\n",
    "\n",
    "cleanedDocs = cleanDocs(medDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "Histologic section of adrenal metastatic disease (hematoxylin and eosin, Г—100). Tumor cells arranged in solid nests infiltrating lymph node tissue.\n",
      "['histologic', 'section', 'adrenal', 'metastatic', 'disease', 'tumor', 'cells', 'arranged', 'solid', 'nests', 'infiltrating', 'lymph', 'node', 'tissue']\n"
     ]
    }
   ],
   "source": [
    "print(len(cleanedDocs))\n",
    "print(medDocs[262]['caption'])\n",
    "print(cleanedDocs[262]['caption'])\n",
    "# cleanedDocs[262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histologic section of adrenal metastatic disease (hematoxylin and eosin, Г—100). Tumor cells arranged in solid nests infiltrating lymph node tissue.\n",
      "Histologic section of adrenal metastatic disease (hematoxylin and eosin, Г—100). Tumor cells arranged in solid nests infiltrating lymph node tissue.\n"
     ]
    }
   ],
   "source": [
    "def createDocList(corpus):\n",
    "    titlesList = []\n",
    "    captionsList = []\n",
    "    for doc in corpus:\n",
    "        titlesList.append(doc['articleTitle'])\n",
    "        captionsList.append(doc['caption'])\n",
    "    return titlesList, captionsList\n",
    "\n",
    "titlesList, captionsList = createDocList(medDocs)\n",
    "print(medDocs[262]['caption'])\n",
    "print(captionsList[262])\n",
    "# titlesList[262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(docList):\n",
    "    vectorizer = TfidfVectorizer(lowercase = False, tokenizer = preprocessContent)\n",
    "    vectors = vectorizer.fit_transform(docList)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return denselist, df\n",
    "\n",
    "title_tfidf, title_df = computeTFIDF(titlesList)\n",
    "caption_tfidf, caption_df = computeTFIDF(captionsList)\n",
    "caption_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(title_tfidf[0])\n",
    "title_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
