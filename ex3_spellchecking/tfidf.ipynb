{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the List of Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching we receive a huge list of results, then we have to rank the results and return the most informative!\n",
    "\n",
    "## Number of overlapping words:\n",
    "- not normalized by length of document\n",
    "\n",
    "## Jaccard Coefficient\n",
    "- $ |\\space X \\cap Y \\space|\\space  /\\space  |\\space X \\cup Y \\space | $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/pgencheva/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categories:  ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "\n",
      "Housing articles: ['test/18911', 'test/19875', 'test/20106', 'test/20116', 'training/1035', 'training/1036', 'training/11170', 'training/11665', 'training/29', 'training/3105', 'training/3708', 'training/3720', 'training/3723', 'training/3898', 'training/5883', 'training/5886', 'training/6000', 'training/6067', 'training/6197', 'training/9615']\n",
      "\n",
      "Words in an arbitrary article: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "# NLTK supports access to different datasets https://www.nltk.org/book/ch02.html\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "print(\"\\nCategories: \", reuters.categories())\n",
    "\n",
    "housing_articles = reuters.fileids('housing')\n",
    "print(\"\\nHousing articles:\", housing_articles)\n",
    "\n",
    "print(\"\\nWords in an arbitrary article:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['housing', 'growth', 'next', 'month']\n",
      "Long Document words number: 131\n",
      "Long Document overlap: 3\n",
      "Long Document JC: 0.03571428571428571\n",
      "\n",
      "Short Document words number: 7\n",
      "Short Document overlap: 3\n",
      "Short Document JC: 0.375\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def word_overlap(doc_tokens, query_tokens):\n",
    "    return sum([1 for _tok in query_tokens if _tok in doc_tokens])\n",
    "\n",
    "def jaccard_coeff(doc_tokens, query_tokens):\n",
    "    # naive intersection of sets\n",
    "    return len(set(doc_tokens).intersection(set(query_tokens))) / len(set(doc_tokens).union(set(query_tokens)))\n",
    "\n",
    "query_tokens = tokenizer.tokenize('housing growth next month')\n",
    "print(\"Query: \", query_tokens)\n",
    "print(\"Long Document words number:\", len(reuters.words('training/6067')))\n",
    "print(\"Long Document overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Long Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "\n",
    "short_similar_document = tokenizer.tokenize('Baldrige predicts housing growth next week.')\n",
    "print(\"\\nShort Document words number:\", len(short_similar_document))\n",
    "print(\"Short Document overlap:\", word_overlap(query_tokens, short_similar_document))\n",
    "print(\"Short Document JC:\", jaccard_coeff(query_tokens, short_similar_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__But we also want to__:\n",
    "- Give _more weight_ to _less frequent words_ in the documents - __Balridge, prices__\n",
    "- Give _less weight_ to _more frequent words_ in the documents - __how, much, housing, to__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document word overlap: 13\n",
      "Document JC: 0.06593406593406594\n",
      "Document Content: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "query_tokens = tokenizer.tokenize('how much will the housing go up in the next month according to Balridge?')\n",
    "print(\"Document word overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document Content:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - Term Frequency- Inverted Document Frequency\n",
    "- View documents as __Bags Of Words__\n",
    "- Mary lent John some money. = John lent Mary some money.\n",
    "- Formula: \n",
    "\n",
    "$$TF * IDF (word, document) = (1+log(tf(word, document)) * log(\\frac{n}{df(word)})$$\n",
    "- n - total number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "- __Frequency of word in a document (here, raw count)__\n",
    "- __0 if the term is not met in the document!!!__\n",
    "- Relevance does not increase proportionally with frequency -> __log (base of 10)__\n",
    "- Makes TF-IDF __increase with the number of occurrences__ within a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>34</td>\n",
       "      <td>2.531479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>31</td>\n",
       "      <td>2.491362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>21</td>\n",
       "      <td>2.322219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pct</td>\n",
       "      <td>15</td>\n",
       "      <td>2.176091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.113943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token  freq        tf\n",
       "0     .    34  2.531479\n",
       "1     ,    31  2.491362\n",
       "2    in    21  2.322219\n",
       "3   pct    15  2.176091\n",
       "4     1    13  2.113943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>689</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>below</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>level</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>687</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token  freq   tf\n",
       "110    689     1  1.0\n",
       "111     11     1  1.0\n",
       "112  below     1  1.0\n",
       "113  level     1  1.0\n",
       "114    687     1  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pandas\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(Counter(reuters.words('test/20116')).most_common(), columns=['token', 'freq'])\n",
    "df['tf'] = 1 + np.log10(df['freq'])\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency\n",
    "- __Number of documents containing the word__ - an inversed measure of significance\n",
    "- Logarithm with base 10 dampens the effect of IDF\n",
    "- Affects ranking of queries with __at least 2 terms__\n",
    "- Makes TFIDF __increase with the rarity of the term in the collection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>.</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0.022276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>the</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>to</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>a</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  doc_freq       idf\n",
       "84     .        20  0.000000\n",
       "164    ,        19  0.022276\n",
       "185  the        17  0.070581\n",
       "160   to        17  0.070581\n",
       "186    a        16  0.096910"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>spurred</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>56th</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>10th</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>SAY</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  doc_freq      idf\n",
       "463  spurred         1  1.30103\n",
       "464       26         1  1.30103\n",
       "465     56th         1  1.30103\n",
       "457     10th         1  1.30103\n",
       "832      SAY         1  1.30103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "document_frequency = defaultdict(lambda: 0)\n",
    "for fileid in housing_articles:\n",
    "    for _word in set(reuters.words(fileid)):\n",
    "        document_frequency[_word] += 1\n",
    "\n",
    "idf_df = pd.DataFrame(list(document_frequency.items()), columns=['word', 'doc_freq'])\n",
    "idf_df['idf'] = np.log10(len(housing_articles)/idf_df['doc_freq'])\n",
    "idf_df.sort_values(by=['idf'], inplace=True)\n",
    "idf_df.head()\n",
    "idf_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we estimate score for a document D w.r.t. a query Q, __summing over tfidf scores of the word in both D and Q__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['housing', 'growth', 'next', 'month']\n",
      "\n",
      "Long Document TF-IDF: 1.2596373105057561\n",
      "\n",
      "Short Document TF-IDF: 2.1627272974976997\n"
     ]
    }
   ],
   "source": [
    "def tfidf_score(query_tokens, document_tokens):\n",
    "    # naive implementation\n",
    "    def tfidf(word):\n",
    "        return (1 + np.log10(document_tokens.count(word))) * idf_df[idf_df['word']==word].iloc[0]['idf']\n",
    "    \n",
    "    overlapping_tokens = set(query_tokens).intersection(set(document_tokens))\n",
    "    return sum([tfidf(_word) for _word in overlapping_tokens])\n",
    "\n",
    "query_tokens = tokenizer.tokenize('housing growth next month')\n",
    "print(\"Query: \", query_tokens)\n",
    "print(\"\\nLong Document TF-IDF:\", tfidf_score(query_tokens, reuters.words('training/6067')))\n",
    "\n",
    "short_similar_document = tokenizer.tokenize('Baldrige predicts housing growth next week.')\n",
    "print(\"\\nShort Document TF-IDF:\", tfidf_score(query_tokens, short_similar_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Now compute the TF-IDF score of the query to the documents:\n",
    "- Query: 'Who was the first man ever to swim around Britain?'\n",
    "- Doc1: 'Ross Edgley, at 33 - first man to swim around Britain'\n",
    "- Doc2: 'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea'\n",
    "- Doc3: 'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?'\n",
    "- Doc4: 'Welcome to the world of strongman swimming | British GQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space\n",
    "- Each document can be represented by a vector, where the terms are the axes of the space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Ross Edgley, at 33 - first man to swim around Britain',\n",
    "    'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea',\n",
    "    'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?',\n",
    "    'Welcome to the world of strongman swimming | British GQ'\n",
    "]\n",
    "query = 'Who was the first man ever to swim around Britain?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.feature_extraction.text:\n",
    "- __CountVectorizer__ - Convert a collection of text documents to a matrix of token counts.\n",
    "- __TfidfVectorizer__ - Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "- Two main methods __fit__ and __transform__ :\n",
    "    - __fit__ goes through the provided documents and __collects the vocabulary__\n",
    "    - __transform__ transforms __documents in text representation to a vector representation__ according to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ross': 15, 'edgley': 7, 'at': 2, '33': 0, 'first': 8, 'man': 12, 'to': 24, 'swim': 20, 'around': 1, 'britain': 3, 'circumnavigate': 6, 'spent': 18, 'months': 13, 'sea': 16, 'get': 9, 'set': 17, 'swimming': 21, 'h2omg': 11, 'can': 5, 'this': 23, 'welcome': 25, 'the': 22, 'world': 26, 'of': 14, 'strongman': 19, 'british': 4, 'gq': 10}\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 install sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(documents)\n",
    "print(count_vectorizer.vocabulary_) # word to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform produces a sparse representations of documents - only values != 0\n",
    "# we need toarray() to preview the whole lists\n",
    "count_vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36984162, 0.36984162, 0.29941866, 0.36984162, 0.36984162,\n",
       "        0.36984162, 0.36984162, 0.        , 0.29941866],\n",
       "       [0.        , 0.48163503, 0.38992506, 0.48163503, 0.        ,\n",
       "        0.48163503, 0.        , 0.        , 0.38992506],\n",
       "       [0.46346838, 0.        , 0.3752176 , 0.        , 0.46346838,\n",
       "        0.        , 0.46346838, 0.46346838, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.77722116, 0.62922751]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'ross': 5,\n",
       " 'edgley': 3,\n",
       " 'at': 1,\n",
       " 'man': 4,\n",
       " 'to': 8,\n",
       " 'swim': 6,\n",
       " 'around': 0,\n",
       " 'britain': 2,\n",
       " 'swimming': 7}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf_vectorizer.fit_transform(documents).toarray()\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing vector similarities\n",
    "- We would like to find documents close to a given document or the closest documents to a query\n",
    "- __Euclidean distance__? - shorter documents will be closer to each other rather than documents talking about same topic\n",
    "- __Cosine similarity__ of the angle between two documents\n",
    "    - divide each vector by its norm to achieve __unit length vectors__\n",
    "    - cosine similarity is simply the __dot product__ of two unit length vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cosine SImilarity](img/cosine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector1 = np.array([1, 0, 0, 1, 2])\n",
    "vector2 = np.array([0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40824829, 0.        , 0.        , 0.40824829, 0.81649658]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.40824829, 0.        , 0.        , 0.40824829, 0.81649658])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "preprocessing.normalize([vector1], norm='l2')\n",
    "vector1 / np.sqrt(sum(vector1**2))\n",
    "\n",
    "unit_vector1 = preprocessing.normalize([vector1], norm='l2')[0]\n",
    "unit_vector2 = preprocessing.normalize([vector2], norm='l2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(unit_vector1, unit_vector2)\n",
    "sum([unit_vector1[i]*unit_vector2[i] for i in range(len(unit_vector1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([vector1, vector2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : calculate the closes document to the query from the previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the first man ever to swim around Britain?\n",
      "1st Closest document: Get Set 4 Swimming - H2OMG! Can this man swim around Britain? Score: 0.8159745583466792\n",
      "2nd Closest document: Ross Edgley, at 33 - first man to swim around Britain Score: 0.7678877104085525\n"
     ]
    }
   ],
   "source": [
    "def get_closest_documents(query, vectorizer, train_corpus_vectors, top_n=2):\n",
    "    \"\"\"Vectorizer should be fit on the documents beforehand.\n",
    "        Returns tuples of (similarity, indexes) of closest documents\"\"\"\n",
    "    # compute similarity to all sentences in the training corpus\n",
    "    similarities = cosine_similarity(vectorizer.transform([query]), train_corpus_vectors).flatten()\n",
    "    # get indexes of top n closest sentences\n",
    "    related_docs_indices = similarities.argsort()[:-top_n-1:-1]\n",
    "    # return tuples of (similarity score, document id)\n",
    "    return [(similarities[idx], idx)  for idx in related_docs_indices]\n",
    "\n",
    "train_corpus_vectors = tfidf_vectorizer.transform(documents)\n",
    "closest_documents = get_closest_documents(query, tfidf_vectorizer, train_corpus_vectors)\n",
    "print('Query:', query)\n",
    "print('1st Closest document: {} Score: {}'.format(documents[closest_documents[0][1]], closest_documents[0][0]))\n",
    "print('2nd Closest document: {} Score: {}'.format(documents[closest_documents[1][1]], closest_documents[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: using the friends corpus try to create an IR chatbot:\n",
    "- User writes a sentences and we find the __closest sentence__ from the transcript\n",
    "- We need to take __the answer__ to that sentence to make a dialogue! \n",
    "- Provide the bot with a __personality__, selecting only the tuple cues, where the answer is by a specific person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>person</th>\n",
       "      <th>gender</th>\n",
       "      <th>original_line</th>\n",
       "      <th>line</th>\n",
       "      <th>metadata</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39776</th>\n",
       "      <td>3974501</td>\n",
       "      <td>2058</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>M</td>\n",
       "      <td>Chandler: Yeah!  No! No! No! Don't fall asleep! Okay, I am going to make you some coffee. (Monica doesn't move as he gets out of bed and as he's heading for the door.) And I probably won't spill coffee grounds all over the kitchen floor.</td>\n",
       "      <td>Yeah! No! No! No! Don't fall asleep! Okay, I am going to make you some coffee. And I probably won't spill coffee grounds all over the kitchen floor.</td>\n",
       "      <td>Yeah_UH !_! No_UH !_! No_UH !_! No_UH !_! Do_VD0 n't_XX fall_VVI asleep_JJ !_! Okay_RR I_PPIS1 am_VBM going_VVGK to_TO make_VVI you_PPY some_DD coffee_NN1 ._. And_CC I_PPIS1 probably_RR wo_VM n't_XX spill_VVI coffee_NN1 grounds_NN2 all_RR over_II the_AT kitchen_NN1 floor_NN1 ._.</td>\n",
       "      <td>0712.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54331</th>\n",
       "      <td>5430001</td>\n",
       "      <td>2762</td>\n",
       "      <td>MONICA</td>\n",
       "      <td>F</td>\n",
       "      <td>Monica: You just wanna stay home so you can make a move on Joey!</td>\n",
       "      <td>You just wanna stay home so you can make a move on Joey!</td>\n",
       "      <td>You_PPY just_RR wan_VVI na_TO stay_VVI home_RL so_CS you_PPY can_VM make_VVI a_AT1 move_NN1 on_II Joey_NP1 !_!</td>\n",
       "      <td>0920.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22751</th>\n",
       "      <td>2272001</td>\n",
       "      <td>1165</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>M</td>\n",
       "      <td>Chandler: Are you serious?</td>\n",
       "      <td>Are you serious?</td>\n",
       "      <td>Are_VBR you_PPY serious_JJ ?_?</td>\n",
       "      <td>0419.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22384</th>\n",
       "      <td>2235301</td>\n",
       "      <td>1147</td>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoebe: I'm telling it! I'm telling it!</td>\n",
       "      <td>I'm telling it! I'm telling it!</td>\n",
       "      <td>I_PPIS1 'm_VBM telling_VVG it_PPH1 !_! I_PPIS1 'm_VBM telling_VVG it_PPH1 !_!</td>\n",
       "      <td>0417.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60024</th>\n",
       "      <td>5999301</td>\n",
       "      <td>3003</td>\n",
       "      <td>RACHEL</td>\n",
       "      <td>F</td>\n",
       "      <td>Rachel: What? Maybe I put it in here . Oh, oh, it's not in there! Oh, no! I must have packed it in one of these boxes!</td>\n",
       "      <td>What? Maybe I put it in here . Oh, oh, it's not in there! Oh, no! I must have packed it in one of these boxes!</td>\n",
       "      <td>What_DDQ ?_? Maybe_RR I_PPIS1 put_VV0 it_PPH1 in_II here_RL ._. Oh_UH oh_UH it_PPH1 's_VBZ not_XX in_II there_RL !_! Oh_UH no_UH !_! I_PPIS1 must_VM have_VHI packed_VVN it_PPH1 in_II one_MC1 of_IO these_DD2 boxes_NN2 !_!</td>\n",
       "      <td>1016.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id scene_id    person gender  \\\n",
       "39776  3974501  2058     CHANDLER  M       \n",
       "54331  5430001  2762     MONICA    F       \n",
       "22751  2272001  1165     CHANDLER  M       \n",
       "22384  2235301  1147     PHOEBE    F       \n",
       "60024  5999301  3003     RACHEL    F       \n",
       "\n",
       "                                                                                                                                                                                                                                       original_line  \\\n",
       "39776  Chandler: Yeah!  No! No! No! Don't fall asleep! Okay, I am going to make you some coffee. (Monica doesn't move as he gets out of bed and as he's heading for the door.) And I probably won't spill coffee grounds all over the kitchen floor.   \n",
       "54331  Monica: You just wanna stay home so you can make a move on Joey!                                                                                                                                                                                \n",
       "22751  Chandler: Are you serious?                                                                                                                                                                                                                      \n",
       "22384  Phoebe: I'm telling it! I'm telling it!                                                                                                                                                                                                         \n",
       "60024  Rachel: What? Maybe I put it in here . Oh, oh, it's not in there! Oh, no! I must have packed it in one of these boxes!                                                                                                                          \n",
       "\n",
       "                                                                                                                                                       line  \\\n",
       "39776  Yeah! No! No! No! Don't fall asleep! Okay, I am going to make you some coffee. And I probably won't spill coffee grounds all over the kitchen floor.   \n",
       "54331  You just wanna stay home so you can make a move on Joey!                                                                                               \n",
       "22751  Are you serious?                                                                                                                                       \n",
       "22384  I'm telling it! I'm telling it!                                                                                                                        \n",
       "60024  What? Maybe I put it in here . Oh, oh, it's not in there! Oh, no! I must have packed it in one of these boxes!                                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                      metadata  \\\n",
       "39776  Yeah_UH !_! No_UH !_! No_UH !_! No_UH !_! Do_VD0 n't_XX fall_VVI asleep_JJ !_! Okay_RR I_PPIS1 am_VBM going_VVGK to_TO make_VVI you_PPY some_DD coffee_NN1 ._. And_CC I_PPIS1 probably_RR wo_VM n't_XX spill_VVI coffee_NN1 grounds_NN2 all_RR over_II the_AT kitchen_NN1 floor_NN1 ._.   \n",
       "54331  You_PPY just_RR wan_VVI na_TO stay_VVI home_RL so_CS you_PPY can_VM make_VVI a_AT1 move_NN1 on_II Joey_NP1 !_!                                                                                                                                                                            \n",
       "22751  Are_VBR you_PPY serious_JJ ?_?                                                                                                                                                                                                                                                            \n",
       "22384  I_PPIS1 'm_VBM telling_VVG it_PPH1 !_! I_PPIS1 'm_VBM telling_VVG it_PPH1 !_!                                                                                                                                                                                                             \n",
       "60024  What_DDQ ?_? Maybe_RR I_PPIS1 put_VV0 it_PPH1 in_II here_RL ._. Oh_UH oh_UH it_PPH1 's_VBZ not_XX in_II there_RL !_! Oh_UH no_UH !_! I_PPIS1 must_VM have_VHI packed_VVN it_PPH1 in_II one_MC1 of_IO these_DD2 boxes_NN2 !_!                                                              \n",
       "\n",
       "       filename  \n",
       "39776  0712.txt  \n",
       "54331  0920.txt  \n",
       "22751  0419.txt  \n",
       "22384  0417.txt  \n",
       "60024  1016.txt  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "friends_corpus = pd.read_csv(\"data/friends-final.txt\", sep='\\t')\n",
    "friends_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>There's nothing to tell! He's just some guy I work with!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOEY</td>\n",
       "      <td>C'mon, you're going out with the guy! There's gotta be something wrong with him!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Just, 'cause, I don't want her to go through what I went through with Carl- oh!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ALL</td>\n",
       "      <td>Oh, yeah. Had that dream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Then I look down, and I realize there's a phone... there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     person  \\\n",
       "0  MONICA     \n",
       "1  JOEY       \n",
       "2  CHANDLER   \n",
       "3  PHOEBE     \n",
       "4  PHOEBE     \n",
       "5  MONICA     \n",
       "6  CHANDLER   \n",
       "7  CHANDLER   \n",
       "8  ALL        \n",
       "9  CHANDLER   \n",
       "\n",
       "                                                                                                                  line  \n",
       "0  There's nothing to tell! He's just some guy I work with!                                                             \n",
       "1  C'mon, you're going out with the guy! There's gotta be something wrong with him!                                     \n",
       "2  Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?                                               \n",
       "3  Wait, does he eat chalk?                                                                                             \n",
       "4  Just, 'cause, I don't want her to go through what I went through with Carl- oh!                                      \n",
       "5  Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.         \n",
       "6  Sounds like a date to me.                                                                                            \n",
       "7  Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.  \n",
       "8  Oh, yeah. Had that dream.                                                                                            \n",
       "9  Then I look down, and I realize there's a phone... there.                                                            "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example conversation\n",
    "friends_corpus[friends_corpus['scene_id']=='1'][['person', 'line']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60849, 15032)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer().fit(friends_corpus.line)\n",
    "train_corpus = vectorizer.transform(friends_corpus.line)\n",
    "train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence:  C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "Its vector representation:  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "An id of a word from the sentence:  14756\n",
      "The word tf-idf score:  0.41270625402865396\n"
     ]
    }
   ],
   "source": [
    "print(\"First sentence: \", friends_corpus.line.values[1])\n",
    "print(\"Its vector representation: \", train_corpus[1].toarray())\n",
    "print(\"An id of a word from the sentence: \", vectorizer.vocabulary_['with'])\n",
    "print(\"The word tf-idf score: \", train_corpus[1].toarray()[0][vectorizer.vocabulary_['with']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8125, 5411)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the previous line, which the cue follows in the dialogue\n",
    "friends_corpus['previous_line'] = ['DUMMY PREVIOUS LINE'] + friends_corpus['line'].values[:-1].tolist()\n",
    "# select only the cues which are made by JOEY\n",
    "joey_line_tuples = friends_corpus[friends_corpus.person == 'JOEY']\n",
    "# create a vectorizer and training space of the documents in the vector space\n",
    "joey_vectorizer = TfidfVectorizer().fit(joey_line_tuples.previous_line)\n",
    "joey_train_corpus = joey_vectorizer.transform(joey_line_tuples.previous_line)\n",
    "joey_train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest_utterance(cue, vectorizer,  train_corpus, top_n=5):\n",
    "    # compute similarity to all sentences in the training corpus\n",
    "    similarities = cosine_similarity(vectorizer.transform([cue]), train_corpus).flatten()\n",
    "    # get indexes of top 5 clocest sentences\n",
    "    related_docs_indices = similarities.argsort()[:-top_n:-1]\n",
    "    # return tuples of (similarity score, sentence)\n",
    "    return [(similarities[idx], joey_line_tuples['line'].values[idx]) \n",
    "                for idx in related_docs_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,\n",
       "  \"Joey Tribbiani! From the wall! Okay, maybe this will jog your memory, huh? Huh? Okay eh-ah-anyway, I'm ready to go back up on the wall I'm the star of a new TV show.\"),\n",
       " (1.0,\n",
       "  \"Oh, hi, I'm Joey. My stupid friends are buying this house. Who are you?\"),\n",
       " (0.7527478944848099, 'Me.'),\n",
       " (0.7527478944848099, \"Alright. I'll give you one hint. Warren Beatty.\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_utterance('who are you?', joey_vectorizer, joey_train_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}