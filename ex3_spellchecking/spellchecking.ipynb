{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description and pre-requisites:\n",
    "## Should have a list of words at hand\n",
    "- Some public list of all words in a language\n",
    "    - Google NGrams - http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
    "    - Project Gutenberg https://www.gutenberg.org/catalog/\n",
    "- Words encountered in indexed documents (can also contain errors)\n",
    "    - Manually add domain specific words\n",
    "    \n",
    "## Finding the correct word - Edit Distance\n",
    "- Measure the number of single-character edits required to change one word into another\n",
    "- Edits are: insert, delete, replace\n",
    "- Solved with dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Distance Algorithm:<br>\n",
    "1. Initialize the matrix with zeros\n",
    "2. Initialize 1st row and column with numbers from 0 to N and  from 0 to M\n",
    "    * this is the cost of inserting a letter, when starting from the empty string\n",
    "3. The value of each of the remaining cells is: <br>\n",
    "<b>m[i, j] = min(<br>\n",
    "            m[i-1,j] + 1, # insert in s1\n",
    "            m[i,j-1] + 1, # insert in s2\n",
    "            m[i-1,j-1] + (0 if s1[i]==s2[j] else 1) # replace\n",
    ")</b> <br>\n",
    "4. The min edit distance is at m[-1,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |S|A|T|U|R|D|A|Y|\n",
    "-|-|-|-|-|-|-|-|-|-|\n",
    " |0|1|2|3|4|5|6|7|8|\n",
    "S|1|0|1|2|3|4|5|6|7|\n",
    "U|2|1|1|2|2|3|4|5|6|\n",
    "N|3|2|2|2|3|3|4|5|6|\n",
    "D|4|3|3|3|3|4|3|4|5|\n",
    "A|5|4|3|4|4|4|4|3|4|\n",
    "Y|6|5|4|4|5|5|5|4|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.__ : Compute the edit distance between the words elephant and relevant. <br/>\n",
    "__Exercise 2.__ : Implement the minimum edit distance algorithm.\n",
    "\n",
    "Some corpora of misspellings can be found here: https://www.dcs.bbk.ac.uk/~ROGER/corpora.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Return the minumum edit distance between the two words\n",
    "    \"\"\"\n",
    "    size_x = len(s1) + 1\n",
    "    size_y = len(s2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y), dtype=int)\n",
    "    for i in range(size_x):\n",
    "        matrix[i, 0] = i\n",
    "    for i in range(size_y):\n",
    "        matrix[0, i] = i\n",
    "\n",
    "    for i in range(1, size_x):\n",
    "        for j in range(1, size_y):\n",
    "            matrix[i,j] = min(matrix[i-1, j] + 1, matrix[i, j-1] + 1,\n",
    "                             matrix[i-1, j-1] if s1[i-1] == s2[j-1] else matrix[i-1, j-1]+1)\n",
    "    return (matrix[-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('lullaby', 'lollipop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants:\n",
    "- Weighted Edit Distance\n",
    "- Keeping a backtrack of the symbol alignments\n",
    "- If a backtrack is not needed we can use only two matrix rows (memory reduced from O(nm) to O(2m))\n",
    "\n",
    "## Some implementations:\n",
    "- Levenstein Edit distance algorithm for Python in Cython for high performance \n",
    "    - https://github.com/gfairchild/pyxDamerauLevenshtein\n",
    "- Peter Norvig's implementation\n",
    "    - Idea : generate all possible words at edit distance 1 and 2, select those that are most probable (using a language model and priority of edit distance)\n",
    "    - Can incorporate error model (some data with common errors http://aspell.net/test/)\n",
    "    - http://norvig.com/spell-correct.html\n",
    "- Library TextBlob - contains code for training a Spellchecker from text\n",
    "    - https://github.com/sloria/TextBlob/blob/14f22102251ce1f02e8bcb3e74f86c037e3df822/textblob/_text.py#L1322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have good spelling!\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install textblob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"I havv goood speling!\")\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.20000000298023224\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install pyxDamerauLevenshtein\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "print(damerau_levenshtein_distance('smtih', 'smith'))\n",
    "print(normalized_damerau_levenshtein_distance('smtih', 'smith'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Problem with the Edit Distance approach:\n",
    "- Do we need to compute the edit distance to all dictionary terms? - __slow__\n",
    "- Solution 1: \n",
    "    - build __charcter n-gram index__ - n-gram:word\n",
    "        - Example: av -> avocado-> pavement, etc.\n",
    "    - retrieve the postings of each n-gram in the query\n",
    "    - merge postings, getting the words with most n-gram overlap\n",
    "    - car can get corrected to carbon, then to normalize the metric for overlap we use __Jaccard coefficient__:\n",
    "        - $ |\\space X \\cap Y \\space|\\space  /\\space  |\\space X \\cup Y \\space | $ \n",
    "        - If more than a threshold, then the words match\n",
    "- Solution 2:\n",
    "    - having to correct the word lates we will possibly compute edit distance to the words late, latest, latte\n",
    "    - for each variant we will compute the edit distance of the common prefix late\n",
    "    - rather use __tries__: store the whole vocabulary in a large trie, where one node is a symbol and the branches are the possible symbols following it\n",
    "    - __compute the edit distance just once for the same prefix__!\n",
    "    - Reference: http://stevehanov.ca/blog/index.php?id=114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.__ Implement the n-gram overlap spell-checker. Find the best Jaccard coefficient for the chosen dataset. <br>\n",
    "__Exercise 4.__ If you already decided what will be your project, build and tune your own spellchecker (or even one for Bulgarian!):\n",
    "- Choose and implementation and a list of words (Textblob's spellchecker is pretrained on Gutenberg's data)\n",
    "- Run your corpus through the spell-checker and find word, which were corrected, but are spelled correctly\n",
    "- Add them to the corpus of words you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "from string import punctuation\n",
    "from os import scandir\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def preprocess_documents(path):\n",
    "    \"\"\"\n",
    "    Returns a list of tokens for a document's content. \n",
    "    Tokens should not contain punctuation and should be lower-cased.\n",
    "    \"\"\"\n",
    "    tokenized_documents = []\n",
    "    for doc_f in scandir(path):\n",
    "        if not doc_f.is_file():\n",
    "            continue\n",
    "        content = open(doc_f).read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        tokens = []\n",
    "        for _sent in sentences:\n",
    "            sent_tokens = tokenizer.tokenize(_sent)\n",
    "            sent_tokens = [_tok.lower() for _tok in sent_tokens if _tok not in punctuation]\n",
    "            tokens += sent_tokens\n",
    "        tokenized_documents.append(tokens)\n",
    "    return tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newsgroups',\n",
       " 'rec.autos',\n",
       " 'path',\n",
       " 'cantaloupe.srv.cs.cmu.edu',\n",
       " 'magnesium.club.cc.cmu.edu',\n",
       " 'news.sei.cmu.edu',\n",
       " 'fs7.ece.cmu.edu',\n",
       " 'europa.eng.gtefsd.com',\n",
       " 'howland.reston.ans.net',\n",
       " 'ux1.cso.uiuc.edu']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_documents = preprocess_documents('data/mini_newsgroups/rec.autos/')\n",
    "tokenized_documents[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_word_ngrams(word, n):\n",
    "    return [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "        \n",
    "        \n",
    "def build_ngram_postings(n, tokenized_documents):\n",
    "    \"\"\"\n",
    "    Create postings from the tokenized documents - \n",
    "    For each n-gram we want to have the list of words, containing that n-gram. (E.g. av: [avocado, pavement, ...])\n",
    "    \"\"\"\n",
    "    processed_words = set()\n",
    "    postings = defaultdict(lambda: [])\n",
    "    for doc in tokenized_documents:\n",
    "        for token in doc:\n",
    "            if token in processed_words:\n",
    "                continue\n",
    "            else:\n",
    "                processed_words.add(token)\n",
    "                for ngram in get_word_ngrams(token, n):\n",
    "                    postings[ngram].append(token)\n",
    "    return postings\n",
    "\n",
    "def ngram_spellcheck(word, postings, n, jaccard_threshold=0.7):\n",
    "    \"\"\"Return the word from the postings structure, which is closest to the input word.\"\"\"\n",
    "    ngram_postings = sum([postings[ngram] for ngram in get_word_ngrams(word, n)], [])\n",
    "    suggestions = []\n",
    "    not_close = 0\n",
    "    for close_word, matching_ngrams in Counter(ngram_postings).most_common():\n",
    "        jaccard_coeff = matching_ngrams / (len(close_word) - n + len(word) - n)\n",
    "        if jaccard_coeff > jaccard_threshold:\n",
    "            suggestions.append(close_word)\n",
    "        else:\n",
    "            not_close += 1\n",
    "            if not_close > 10:\n",
    "                break\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postings of bi-gram 'ar':\n",
      "['article', 'darren', 'cars', 'hard', 'yard', 'compartment', 'are', 'early', 'area', 'antiauthoritarian']\n",
      "Closest words to enginering:\n",
      "['engineering', 'enginerring']\n"
     ]
    }
   ],
   "source": [
    "postings = build_ngram_postings(2, tokenized_documents)\n",
    "print(\"Postings of bi-gram 'ar':\")\n",
    "print(postings['ar'][:10])\n",
    "print(\"Closest words to enginering:\")\n",
    "print(ngram_spellcheck('enginering', postings, 2, jaccard_threshold=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Building a Grammarly-like spelling/grammar correction for Bulgarian can be a good course project. \n",
    "If anyone is interested, we can discuss how to collect data and how to approach the task._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}