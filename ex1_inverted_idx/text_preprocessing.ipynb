{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main steps for building an index:\n",
    "\n",
    "![Sort-Based-Index](img/index_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nHurricane-force winds have struck central and northern Portugal, leaving 300,000 homes without power.',\n",
       " 'The remnants of Hurricane Leslie swept in overnight on Saturday, with winds gusting up to 176km/h (109mph).',\n",
       " 'Civil defence officials said 27 people suffered minor injuries, with localised flooding, hundreds of trees uprooted and a number of flights cancelled.',\n",
       " 'The storm, one of the most powerful to ever hit the country, is now passing over northern Spain.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "example_document = \"\"\"\n",
    "Hurricane-force winds have struck central and northern Portugal, leaving 300,000 homes without power.\n",
    "\n",
    "The remnants of Hurricane Leslie swept in overnight on Saturday, with winds gusting up to 176km/h (109mph).\n",
    "\n",
    "Civil defence officials said 27 people suffered minor injuries, with localised flooding, hundreds of trees uprooted and a number of flights cancelled.\n",
    "\n",
    "The storm, one of the most powerful to ever hit the country, is now passing over northern Spain.\n",
    "\"\"\"\n",
    "sentences = sent_tokenizer.tokenize(example_document)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace tokenizer:\n",
      "['Hurricane-force', 'winds', 'have', 'struck', 'central', 'and', 'northern', 'Portugal,', 'leaving', '300,000', 'homes', 'without', 'power.']\n",
      "\n",
      "Word Punctuation Tokenizer:\n",
      "['Civil', 'defence', 'officials', 'said', '27', 'people', 'suffered', 'minor', 'injuries', ',', 'with', 'localised', 'flooding', ',', 'hundreds', 'of', 'trees', 'uprooted', 'and', 'a', 'number', 'of', 'flights', 'cancelled', '.']\n",
      "['On', 'a', '$', '50', ',', '000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366', '.', '88', '.']\n",
      "\n",
      "Tree bank tokenizer:\n",
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366.88', '.']\n",
      "['My', 'email', 'is', 'admin', '@', 'sth.com']\n",
      "\n",
      "Tweeter Tokenizer:\n",
      "['My', 'email', 'is', 'admin@sth.com']\n",
      "['Last', 'sunny', 'day', 'before', 'winter', ':)', '#sunisenergy']\n",
      "['Here', 'is', 'my', 'ip', 'address', '192.33', '.', '24.45']\n",
      "['I', \"can't\", 'stop', 'singing', 'that', 'song', '!']\n",
      "['The', 'remnants', 'of', 'Hurricane', 'Leslie', 'swept', 'in', 'overnight', 'on', 'Saturday', ',', 'with', 'winds', 'gusting', 'up', 'to', '176km', '/', 'h', '(', '109mph', ')', '.']\n",
      "['Eugenia', 'e', 'Jack', 'sposi', 'e', 'innamorati', '(', 'ma', 'la', 'magia', 'di', 'Harry', 'e', 'Meghan', 'non', 'c', 'â€™', 'Ã¨', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer, WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "tweet_tok, whsp_tok, wordpunct_tok, treebank_tok = TweetTokenizer(), WhitespaceTokenizer(), WordPunctTokenizer(), TreebankWordTokenizer()\n",
    "\n",
    "print(\"Whitespace tokenizer:\")\n",
    "print(whsp_tok.tokenize(sentences[0]))\n",
    "\n",
    "print(\"\\nWord Punctuation Tokenizer:\")\n",
    "print(wordpunct_tok.tokenize(sentences[2]))\n",
    "print(wordpunct_tok.tokenize('On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.'))\n",
    "\n",
    "print(\"\\nTree bank tokenizer:\")\n",
    "print(treebank_tok.tokenize('On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.'))\n",
    "print(treebank_tok.tokenize('My email is admin@sth.com'))\n",
    "\n",
    "print(\"\\nTweeter Tokenizer:\")\n",
    "print(tweet_tok.tokenize('My email is admin@sth.com'))\n",
    "print(tweet_tok.tokenize('Last sunny day before winter :) #sunisenergy'))\n",
    "print(tweet_tok.tokenize('Here is my ip address 192.33.24.45')) # we might want to fix this\n",
    "print(tweet_tok.tokenize(\"I can't stop singing that song!\")) # we might want to split can't in can, 't instead\n",
    "print(tweet_tok.tokenize(sentences[1])) # maybe we want to keep  km/h together apart from 176?\n",
    "print(tweet_tok.tokenize('Eugenia e Jack sposi e innamorati (ma la magia di Harry e Meghan non câ€™Ã¨)')) # maybe we want language specific tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: play with the tokenizers and find more vulnerabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Depending on the domain and the task at hand, we would like to make text normalization. Some examples of text normalizing steps can include:\n",
    "- Whole chunks of the document might be irrelevant, e.g. html tags.\n",
    "- __Numbers__ - we can remove them or replace them with a common token.\n",
    "- We can do the same for all other token types we don't care about, e.g. URLs, @ mentions, ðŸ˜€, etc.\n",
    "- We can __lower-case__ all words or even use a tool to upper-case all named entities like geographic names.\n",
    "- We might want to add __equivalence classes__ for matching synonyms, abbreviations, etc.\n",
    "- Remove __stopwords__ - the list of stopwords is usually manually curated\n",
    "- Google N-Grams - a huge corpus of statistics, showing frequency of word n-grams in web pages. It contains statistics for many languages and can be used to further examine frequently used words. \n",
    "- __Stemming__ - crude removal of prefixes and suffixes to reduce the word form to match words like car-sharing\n",
    "- __Lemmatiazation__ - replacing the word by its lemma\n",
    "\n",
    "__Exercise__: Find examples when we wouldn't like to normalize text with each of the proposed methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Boris\n",
      "[nltk_data]     Velichkov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Car Car car\n",
      "sharing sharing share\n",
      "will will will\n",
      "bring bring bring\n",
      "a a a\n",
      "whole whole whole\n",
      "new new new\n",
      "era era era\n",
      "to to to\n",
      "the the the\n",
      "automobile automobile automobil\n",
      "industry industry industri\n",
      ". . .\n",
      "Eventually Eventually eventu\n",
      ", , ,\n",
      "it it it\n",
      "may may may\n",
      "even even even\n",
      "decreas decreas decrea\n",
      "air air air\n",
      "pollutions pollution pollut\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = tweet_tok.tokenize('Car sharing will bring a whole new era to the automobile industry. Eventually, it may even decreas air pollutions.')\n",
    "for token in tokens:\n",
    "    print(token, lemmatizer.lemmatize(token), stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Boris\n",
      "[nltk_data]     Velichkov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: explore the dataset from 20 news groups and create another preprocessing method, which will clean the text of the news articles:\n",
    "- Select one of the tokenizers or try to write one of your own\n",
    "- Clean the headings and any common parts\n",
    "- Lowercase the text and see if you might want to remove any token types (e.g. URLs, you might also want to split URLs to meaningful words and use them as tokens)\n",
    "- Explore the most frequent words in the corpus and add more stop-words (depending on your tokenizer you might also need to modify the stopwords, too). Then, remove them from the text.\n",
    "- Have a method variable for choosing lemmatization or stemming\n",
    "- Finally compare how all the steps reduced/modified the index.\n",
    "\n",
    "\n",
    "You will also have to include all the steps for the query words in your query methods, too!\n",
    "- Explore how the quering results changed in both positive and negative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}